{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziad11222/Grocery_Project/blob/main/Graduation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "ov8p8c3QjJdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fafda7-db45-49fa-eaf8-7ba712b320e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "NzXF6xf3qKSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, Conv2DTranspose"
      ],
      "metadata": {
        "id": "p0Sef-J8yjsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import Model\n",
        "\n"
      ],
      "metadata": {
        "id": "R-tip_ATx6To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/thou/\")\n",
        "import logging\n",
        "import os\n",
        "from random import randint\n",
        "from keras import backend as K\n",
        "import random\n",
        "from PIL import Image\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils import to_categorical\n",
        "import pickle"
      ],
      "metadata": {
        "id": "ofuttR05rI6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/My Drive/images.zip' -d '/content/'"
      ],
      "metadata": {
        "id": "y7aTXbz6jk_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bed5e04-f971-4e5d-b70e-2697978354c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/My Drive/images.zip, /content/drive/My Drive/images.zip.zip or /content/drive/My Drive/images.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_t-qNr5w0xnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x '/content/content/ThoughtViz.rar'"
      ],
      "metadata": {
        "id": "1AObCHhJt6BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r thou2.zip /content/thou\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPN9Uia0ov_f",
        "outputId": "9600bf69-00fe-48c0-981f-331f2b4a3367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/thou/ (stored 0%)\n",
            "  adding: content/thou/training/ (stored 0%)\n",
            "  adding: content/thou/training/baseline_acgan_with_eeg.py (deflated 68%)\n",
            "  adding: content/thou/training/baseline_deligan_image.py (deflated 70%)\n",
            "  adding: content/thou/training/__init__.py (stored 0%)\n",
            "  adding: content/thou/training/__pycache__/ (stored 0%)\n",
            "  adding: content/thou/training/__pycache__/__init__.cpython-310.pyc (deflated 27%)\n",
            "  adding: content/thou/training/eeg_classification.py (deflated 62%)\n",
            "  adding: content/thou/training/thoughtviz_with_eeg.py (deflated 69%)\n",
            "  adding: content/thou/training/thoughtviz_with_label.py (deflated 68%)\n",
            "  adding: content/thou/training/thoughtviz_image_with_eeg.py (deflated 70%)\n",
            "  adding: content/thou/training/baseline_acgan_image.py (deflated 70%)\n",
            "  adding: content/thou/training/models/ (stored 0%)\n",
            "  adding: content/thou/training/models/__init__.py (stored 0%)\n",
            "  adding: content/thou/training/models/thoughtviz.py (deflated 81%)\n",
            "  adding: content/thou/training/models/__pycache__/ (stored 0%)\n",
            "  adding: content/thou/training/models/__pycache__/thoughtviz.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/thou/training/models/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
            "  adding: content/thou/training/models/ac_gan.py (deflated 80%)\n",
            "  adding: content/thou/training/models/deligan.py (deflated 80%)\n",
            "  adding: content/thou/training/models/classification.py (deflated 64%)\n",
            "  adding: content/thou/training/baseline_acgan.py (deflated 68%)\n",
            "  adding: content/thou/training/baseline_deligan_with_eeg.py (deflated 69%)\n",
            "  adding: content/thou/training/baseline_deligan.py (deflated 68%)\n",
            "  adding: content/thou/testing/ (stored 0%)\n",
            "  adding: content/thou/testing/test.py (deflated 69%)\n",
            "  adding: content/thou/README.md (deflated 58%)\n",
            "  adding: content/thou/layers/ (stored 0%)\n",
            "  adding: content/thou/layers/__init__.py (stored 0%)\n",
            "  adding: content/thou/layers/__pycache__/ (stored 0%)\n",
            "  adding: content/thou/layers/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
            "  adding: content/thou/layers/__pycache__/mog_layer.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/thou/layers/mog_layer.py (deflated 71%)\n",
            "  adding: content/thou/utils/ (stored 0%)\n",
            "  adding: content/thou/utils/__init__.py (stored 0%)\n",
            "  adding: content/thou/utils/__pycache__/ (stored 0%)\n",
            "  adding: content/thou/utils/__pycache__/eval_utils.cpython-310.pyc (deflated 37%)\n",
            "  adding: content/thou/utils/__pycache__/__init__.cpython-310.pyc (deflated 27%)\n",
            "  adding: content/thou/utils/__pycache__/data_input_util.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/thou/utils/__pycache__/image_utils.cpython-310.pyc (deflated 49%)\n",
            "  adding: content/thou/utils/data_input_util.py (deflated 72%)\n",
            "  adding: content/thou/utils/image_utils.py (deflated 74%)\n",
            "  adding: content/thou/utils/eval_utils.py (deflated 55%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgHb3kA6hF-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "b77a8865-6907-44ba-b6f4-560d390cf5ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "No file or directory found at run_final.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b428667675c4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the pre-trained model from an h5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_final.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpkl_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/data/eeg/char/data.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at run_final.h5"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the pre-trained model from an h5 file\n",
        "model = load_model(\"run_final.h5\")\n",
        "pkl_path = '/content/data/eeg/char/data.pkl'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_data = pickle.load(open(pkl_path, \"rb\"), encoding='bytes')\n"
      ],
      "metadata": {
        "id": "c-wpH3DdmQVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "SxWVBjYAhmoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Load your pre-trained model from an h5 file (replace with your actual model path)\n",
        "model = load_model(\"run_final.h5\")\n",
        "\n",
        "# Plot the model architecture and save it to a file (optional)\n",
        "plot_model(model, to_file=\"model_diagram.png\", show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Display the model diagram (requires Graphviz to be installed)\n",
        "# If you haven't installed Graphviz, you can do so using: !pip install graphviz\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "3zEL4I0mh8My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = inutil.load_char_data('/content/images/Char-Font/', resize_shape=(28, 28))\n",
        "print(\"Loaded Characters Dataset.\", )"
      ],
      "metadata": {
        "id": "hBsSQ0Bzpm8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ayhaja = model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "id": "SALoxJxWiNCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "Jtt-ikoNfVZ9",
        "outputId": "58953543-79a3-4358-ecd9-4c17ed6b03c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.2.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.2.1 namex-0.0.7 optree-0.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "1cc54fa3d6d8466796ab72904d02664c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from random import randint\n",
        "from keras import backend as K\n",
        "import random\n",
        "from PIL import Image\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "import thou.utils.data_input_util as inutil\n",
        "from   thou.training.models.thoughtviz import *\n",
        "from   thou.utils.image_utils import *\n",
        "from   thou.utils.eval_utils import *\n",
        "\n"
      ],
      "metadata": {
        "id": "ioPd8txie105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d2ed76-3235-466d-ac1a-0db31886f4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Downloading inception-2015-12-05.tgz 100.0%\n",
            "Succesfully downloaded inception-2015-12-05.tgz 88931400 bytes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_gan(input_noise_dim, batch_size, epochs, model_save_dir, output_dir, classifier_model_file, **kwargs):\n",
        "    # Function body\n",
        "    # You can access splits_save_dir with kwargs.get('splits_save_dir', default_value)\n",
        "    # Add your existing function logic here\n",
        "\n",
        "    K.set_learning_phase(False)\n",
        "    # folders containing images used for training\n",
        "    imagenet_folder = \"/content/ThoughtViz/training/images/ImageNet-Filtered\"\n",
        "    num_classes = 10\n",
        "\n",
        "    feature_encoding_dim = 100\n",
        "\n",
        "    # load data and compile discriminator, generator models depending on the dataaset\n",
        "    x_train, y_train, x_test, y_test = inutil.load_image_data(imagenet_folder, patch_size=(64, 64))\n",
        "    print(\"Loaded Images Dataset.\", )\n",
        "\n",
        "    g_adam_lr = 0.00003\n",
        "    g_adam_beta_1 = 0.5\n",
        "\n",
        "    d_adam_lr = 0.00005\n",
        "    d_adam_beta_1 = 0.5\n",
        "\n",
        "    c = load_model(classifier_model_file)\n",
        "\n",
        "    d = discriminator_model_rgb((64, 64), c)\n",
        "    d_optim = Adam(lr=d_adam_lr, beta_1=d_adam_beta_1)\n",
        "    d.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=d_optim)\n",
        "    d.trainable = True\n",
        "\n",
        "    g = generator_model_rgb(input_noise_dim, feature_encoding_dim)\n",
        "    g_optim = Adam(lr=g_adam_lr, beta_1=g_adam_beta_1)\n",
        "    g.compile(loss='categorical_crossentropy', optimizer=g_optim)\n",
        "\n",
        "    d_on_g = generator_containing_discriminator(input_noise_dim, feature_encoding_dim, g, d)\n",
        "    d_on_g.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=g_optim)\n",
        "\n",
        "    g.summary()\n",
        "    d.summary()\n",
        "\n",
        "    eeg_data = pickle.load(open(os.path.join(data_dir, 'data.pkl'), \"rb\"))\n",
        "    classifier = load_model(saved_classifier_model_file)\n",
        "    classifier.summary()\n",
        "    x_test = eeg_data[b'x_test']\n",
        "    y_test = eeg_data[b'y_test']\n",
        "    y_test = np.array([np.argmax(y) for y in y_test])\n",
        "    layer_index = 9\n",
        "\n",
        "    # keras way of getting the output from an intermediate layer\n",
        "    get_nth_layer_output = K.function([classifier.layers[0].input], [classifier.layers[layer_index].output])\n",
        "\n",
        "    layer_output = get_nth_layer_output([x_test])[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch is \", epoch)\n",
        "\n",
        "        print(\"Number of batches\", int(x_train.shape[0]/batch_size))\n",
        "\n",
        "        for index in range(int(x_train.shape[0]/batch_size)):\n",
        "            # generate noise from a normal distribution\n",
        "            noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
        "\n",
        "            random_labels = np.random.randint(0, 10, batch_size)\n",
        "\n",
        "            one_hot_vectors = [to_categorical(label, 10) for label in random_labels]\n",
        "\n",
        "            eeg_feature_vectors = np.array([layer_output[random.choice(np.where(y_test == random_label)[0])] for random_label in random_labels])\n",
        "\n",
        "            # get real images and corresponding labels\n",
        "            real_images = x_train[index * batch_size:(index + 1) * batch_size]\n",
        "            real_labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "            # generate fake images using the generator\n",
        "            generated_images = g.predict([noise, eeg_feature_vectors], verbose=0)\n",
        "\n",
        "            # discriminator loss of real images\n",
        "            d_loss_real = d.train_on_batch(real_images, [np.array([1] * batch_size), np.array(real_labels)])\n",
        "            # discriminator loss of fake images\n",
        "            d_loss_fake = d.train_on_batch(generated_images, [np.array([0] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
        "            d_loss = (d_loss_fake[0] + d_loss_real[0]) * 0.5\n",
        "\n",
        "            d.trainable = False\n",
        "            # generator loss\n",
        "            g_loss = d_on_g.train_on_batch([noise, eeg_feature_vectors], [np.array([1] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
        "            d.trainable = True\n",
        "\n",
        "        # save generated images at intermediate stages of training\n",
        "        if epoch % 100 == 0:\n",
        "            image = combine_rgb_images(generated_images)\n",
        "            image = image * 255.0\n",
        "            img_save_path = os.path.join(output_dir, str(epoch) + \"_g\" + \".png\")\n",
        "            Image.fromarray(image.astype(np.uint8)).save(img_save_path)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            test_image_count = 50000\n",
        "            test_noise = np.random.uniform(-1, 1, (test_image_count, input_noise_dim))\n",
        "            test_labels = np.random.randint(0, 10, test_image_count)\n",
        "\n",
        "            eeg_feature_vectors_test = np.array([layer_output[random.choice(np.where(y_test == test_label)[0])] for test_label in test_labels])\n",
        "            test_images = g.predict([test_noise, eeg_feature_vectors_test], verbose=0)\n",
        "            test_images = test_images * 255.0\n",
        "            inception_score = get_inception_score([test_image for test_image in test_images], splits=10)\n",
        "\n",
        "        print(\"Epoch %d d_loss : %f\" % (epoch, d_loss))\n",
        "        print(\"Epoch %d g_loss : %f\" % (epoch, g_loss[0]))\n",
        "        print(\"Epoch %d inception_score : %f\" % (epoch, inception_score[0]))\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            # save generator and discriminator models along with the weights\n",
        "            g.save(os.path.join(model_save_dir, 'generator_' + str(epoch)), overwrite=True, include_optimizer=True)\n",
        "            d.save(os.path.join(model_save_dir, 'discriminator_' + str(epoch)), overwrite=True, include_optimizer=True)\n",
        "\n",
        "\n",
        "def train():\n",
        "    dataset = 'Image'\n",
        "    batch_size = 100\n",
        "    run_id = 1\n",
        "    epochs = 10000\n",
        "    eeg_data_dir = '/content/ThoughtViz/data/eeg/image/data.pkl'\n",
        "    eeg_classifier_model_file = '/content/ThoughtViz/models/eeg_models/image/run_final.h5'\n",
        "    output_dir = '/content/content'\n",
        "    classifier_model_file = '/content/thou/training/models/classification.py'\n",
        "    model_save_dir = os.path.join('./saved_models/thoughtviz_image_with_eeg/', dataset, 'run_' + str(run_id))\n",
        "    train_gan(input_noise_dim=100, batch_size=batch_size, epochs=epochs, splits_save_dir=eeg_data_dir, saved_classifier_model_file=eeg_classifier_model_file, model_save_dir=model_save_dir, output_dir=output_dir, classifier_model_file=classifier_model_file)\n",
        "\n",
        "    if not os.path.exists(model_save_dir):\n",
        "        os.makedirs(model_save_dir)\n",
        "\n",
        "    output_dir = os.path.join('./outputs/thoughtviz_image_with_eeg/', dataset, 'run_' + str(run_id))\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    classifier_model_file = os.path.join('./trained_classifier_models', 'classifier_' + dataset.lower() + '.h5')\n",
        "\n",
        "    eeg_data_dir = os.path.join('../data/eeg/', dataset.lower())\n",
        "    eeg_classifier_model_file = os.path.join('../models/eeg_models', dataset.lower(), 'run_final.h5')\n",
        "\n",
        "    train_gan(input_noise_dim=100, batch_size=batch_size, epochs=epochs, splits_save_dir=eeg_data_dir, saved_classifier_model_file=eeg_classifier_model_file, model_save_dir=model_save_dir, output_dir=output_dir, classifier_model_file=classifier_model_file)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "_wnE1Mj1VuBP",
        "outputId": "b3e0bac4-5494-4bb8-b572-91a041d7808e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1796 1796\n",
            "Loaded Images Dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Unable to open file (file signature not found)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-a5757924a8c3>\u001b[0m in \u001b[0;36m<cell line: 140>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-a5757924a8c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mclassifier_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/thou/training/models/classification.py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mmodel_save_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./saved_models/thoughtviz_image_with_eeg/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtrain_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_noise_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits_save_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meeg_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_classifier_model_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meeg_classifier_model_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_model_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-a5757924a8c3>\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(input_noise_dim, batch_size, epochs, model_save_dir, output_dir, classifier_model_file, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0md_adam_beta_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_model_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "import keras.backend as K\n",
        "from PIL import Image\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from layers.mog_layer import *\n",
        "from utils.image_utils import *\n",
        "\n",
        "\n",
        "class Tests():\n",
        "\n",
        "    def test_deligan_baseline(self, generator_model):\n",
        "        K.set_learning_phase(False)\n",
        "\n",
        "        input_noise_dim = 100\n",
        "        batch_size = 50\n",
        "\n",
        "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
        "\n",
        "        random_labels = np.random.randint(0, 10, batch_size)\n",
        "\n",
        "        conditioned_noise = []\n",
        "        for i in range(batch_size):\n",
        "            conditioned_noise.append(np.append(noise[i], to_categorical(random_labels[i], 10)))\n",
        "        conditioned_noise = np.array(conditioned_noise)\n",
        "\n",
        "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
        "\n",
        "        # generate images using the generator\n",
        "        generated_images = g.predict(conditioned_noise, verbose=0)\n",
        "\n",
        "        image = combine_rgb_images(generated_images)\n",
        "        image = image * 127.5 + 127.5\n",
        "        img = Image.fromarray(image.astype(np.uint8))\n",
        "        img.show()\n",
        "\n",
        "    def test_deligan_final(self, generator_model, classifier_model, eeg_pkl_file):\n",
        "        K.set_learning_phase(False)\n",
        "\n",
        "        # load EEG data\n",
        "        eeg_data = pickle.load(open(eeg_pkl_file, \"rb\"), encoding='bytes')\n",
        "        classifier = load_model(classifier_model)\n",
        "\n",
        "        x_test = eeg_data[b'x_test']\n",
        "        y_test = eeg_data[b'y_test']\n",
        "        y_test = [np.argmax(y) for y in y_test]\n",
        "        layer_index = 9\n",
        "\n",
        "        # keras way of getting the output from an intermediate layer\n",
        "        get_nth_layer_output = K.function([classifier.layers[0].input], [classifier.layers[layer_index].output])\n",
        "\n",
        "        layer_output = get_nth_layer_output([x_test])[0]\n",
        "\n",
        "        input_noise_dim = 100\n",
        "        batch_size = 50\n",
        "\n",
        "        noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
        "\n",
        "        random_labels = np.random.randint(0, 10, batch_size)\n",
        "\n",
        "        eeg_feature_vectors = [layer_output[random.choice(np.where(y_test == random_label)[0])] for random_label in random_labels]\n",
        "\n",
        "        noises, conditionings = [], []\n",
        "        for i in range(batch_size):\n",
        "            noises.append(noise[i])\n",
        "            conditionings.append(eeg_feature_vectors[i])\n",
        "\n",
        "        g = load_model(generator_model, custom_objects={'MoGLayer': MoGLayer})\n",
        "\n",
        "        # generate images using the generator\n",
        "        generated_images = g.predict([np.array(noises), np.array(conditionings)], verbose=0)\n",
        "\n",
        "        image = combine_rgb_images(generated_images)\n",
        "        image = image * 127.5 + 127.5\n",
        "        img = Image.fromarray(image.astype(np.uint8))\n",
        "        img.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tests = Tests()\n",
        "    #tests.test_deligan_baseline('../models/gan_models/baseline/deligan/image/generator.model')\n",
        "    tests.test_deligan_final('../models/gan_models/final/image/generator.model',\n",
        "                       '/content/ThoughtViz/models/eeg_models/image/run_final.h5',\n",
        "                       '/content/ThoughtViz/data/eeg/image/data.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "5gZTUdNH1caQ",
        "outputId": "0f2eb3dd-9e07-4da7-a46e-934b8ac9e5de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "Exception encountered when calling layer 'conv2d_3' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:CPU:0}} The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW [Op:Conv2D] name: \n\nCall arguments received by layer 'conv2d_3' (type Conv2D):\n  • inputs=tf.Tensor(shape=(5706, 1, 9, 25), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2c42d7629f74>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m#tests.test_deligan_baseline('../models/gan_models/baseline/deligan/image/generator.model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     tests.test_deligan_final('../models/gan_models/final/image/generator.model',\n\u001b[0m\u001b[1;32m     85\u001b[0m                        \u001b[0;34m'/content/ThoughtViz/models/eeg_models/image/run_final.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                        '/content/ThoughtViz/data/eeg/image/data.pkl')\n",
            "\u001b[0;32m<ipython-input-26-2c42d7629f74>\u001b[0m in \u001b[0;36mtest_deligan_final\u001b[0;34m(self, generator_model, classifier_model, eeg_pkl_file)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mget_nth_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nth_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0minput_noise_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(model_inputs)\u001b[0m\n\u001b[1;32m   4657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4658\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4659\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4660\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4661\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5882\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5883\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Exception encountered when calling layer 'conv2d_3' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:CPU:0}} The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW [Op:Conv2D] name: \n\nCall arguments received by layer 'conv2d_3' (type Conv2D):\n  • inputs=tf.Tensor(shape=(5706, 1, 9, 25), dtype=float32)"
          ]
        }
      ]
    }
  ]
}